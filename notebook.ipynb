{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOL2nU8W94Pg"
      },
      "source": [
        "# NLP Project - Jonas Luerkens & Esther Kaltwasser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esVZLd73Bb0E"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "okMoSeaGOUga"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!git clone https://github.com/esthikalt/NLP_Project.git\n",
        "%cd NLP_Project\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjqDNEzD9g0m"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "from datapreparation import load_and_prepare_data\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding, DataCollatorForLanguageModeling, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from datasets import Dataset\n",
        "import re\n",
        "from tqdm import tqdm # Loading bar\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Disabling parallelism for tokenizers to avoid warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ajaAd5jVBEdm"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "train_ds, val_ds, test_ds, label_list, label2id, id2label = load_and_prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d9kpcv2dLH7S"
      },
      "outputs": [],
      "source": [
        "# Load the models and tokenizer for task 1 and 2\n",
        "\n",
        "distilgpt2_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\") # Same tokenizer as distilgpt2\n",
        "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3yEkhmVCNQE"
      },
      "outputs": [],
      "source": [
        "# Setting the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yZocsEiB_Tk"
      },
      "outputs": [],
      "source": [
        "# Metrics can vary across runs due to stochastic training;\n",
        "# Setting seeds helps ensure that reported results can be replicated\n",
        "# It fixes the random number generators so the model initializes and trains in the same way each time\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88aL6xjWpYL"
      },
      "source": [
        "### Helping functions for displaying images and the result table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwecZyfmWvjs"
      },
      "outputs": [],
      "source": [
        "# Helping functions\n",
        "\n",
        "def plot_grafics(path1, path2, dpi=200):\n",
        "  img1 = mpimg.imread(path1)\n",
        "  img2 = mpimg.imread(path2)\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 7), dpi=dpi)\n",
        "\n",
        "  # Grafic 1\n",
        "  ax[0].imshow(img1)\n",
        "  ax[0].axis('off')\n",
        "\n",
        "  # Grafic 2\n",
        "  ax[1].imshow(img2)\n",
        "  ax[1].axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def show_results_table():\n",
        "    # Task 1:\n",
        "    gpt2_prompt_acc   = \"20.04%\"\n",
        "    gpt2_prompt_f1    = \"15.11%\"\n",
        "    gpt2_prompt_time  = \"~0.6 s\"\n",
        "    gpt2_prompt_mem   = \"~500 MB\"\n",
        "\n",
        "    distilgpt2_prompt_acc  = \"38.38%\"\n",
        "    distilgpt2_prompt_f1   = \"17.91%\"\n",
        "    distilgpt2_prompt_time = \"~0.4 s\"\n",
        "    distilgpt2_prompt_mem  = \"~330 MB\"\n",
        "\n",
        "    # Task 2:\n",
        "    gpt2_ft_acc   = \"73.99%\"\n",
        "    gpt2_ft_f1    = \"56.43%\"\n",
        "    gpt2_ft_time  = \"~1.8 s\"\n",
        "    gpt2_ft_mem   = \"~506.5 MB\"\n",
        "\n",
        "    distilgpt2_ft_acc  = \"69.22%\"\n",
        "    distilgpt2_ft_f1   = \"54.24%\"\n",
        "    distilgpt2_ft_time = \"~0.9 s\"\n",
        "    distilgpt2_ft_mem  = \"~335 MB\"\n",
        "\n",
        "    # Task 3:\n",
        "    distilbert_acc  = \"82.26 %\"\n",
        "    distilbert_f1   = \"72.86 %\"\n",
        "    distilbert_time = \"~0.02 s\"\n",
        "    distilbert_mem  = \"~260 MB\"\n",
        "\n",
        "    data = {\n",
        "        \"Metric\": [\n",
        "            \"Accuracy\",\n",
        "            \"Macro F1-Score\",\n",
        "            \"Inference Time (per Mail)\",\n",
        "            \"Model Size (Memory)\"\n",
        "        ],\n",
        "        \"GPT-2 (Prompting)\": [\n",
        "            gpt2_prompt_acc, gpt2_prompt_f1, gpt2_prompt_time, gpt2_prompt_mem\n",
        "        ],\n",
        "        \"DistilGPT-2 (Prompting)\": [\n",
        "            distilgpt2_prompt_acc, distilgpt2_prompt_f1, distilgpt2_prompt_time, distilgpt2_prompt_mem\n",
        "        ],\n",
        "        \"GPT-2 (Fine-Tuning)\": [\n",
        "            gpt2_ft_acc, gpt2_ft_f1, gpt2_ft_time, gpt2_ft_mem\n",
        "        ],\n",
        "        \"DistilGPT-2 (Fine-Tuning)\": [\n",
        "            distilgpt2_ft_acc, distilgpt2_ft_f1, distilgpt2_ft_time, distilgpt2_ft_mem\n",
        "        ],\n",
        "        \"DistilBERT (Classifier)\": [\n",
        "            distilbert_acc, distilbert_f1, distilbert_time, distilbert_mem\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_results = pd.DataFrame(data)\n",
        "    df_results.set_index(\"Metric\", inplace=True)\n",
        "\n",
        "    display(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LERvjo1UB79M"
      },
      "source": [
        "## Agent 1: Routing with prompting using GPT-2 (frozen model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr42eipZzP71"
      },
      "source": [
        "### Prompt function and prompt\n",
        "Here we define the few-shot prompt for the model. It includes one short, precise example for one email for each department."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1Yy1newBpKM"
      },
      "outputs": [],
      "source": [
        "# Few-shot learning prompt (1 email per department)\n",
        "\n",
        "prompt = (\n",
        "    \"Classify the email ONLY into the following categories: Technical Support, Customer Service, Billing and Payments, Sales and Pre-Sales, General Inquiry.\\n\\n\"\n",
        "    \"Subject: Return item | Body: I want to send this product back. -> Department: Customer Service\\n\"\n",
        "    \"Subject: Invoice missing | Body: I need my bill for July. -> Department: Billing and Payments\\n\"\n",
        "    \"Subject: Discount inquiry | Body: Do you have bulk pricing? -> Department: Sales and Pre-Sales\\n\"\n",
        "    \"Subject: Opening hours | Body: When are you open? -> Department: General Inquiry\\n\"\n",
        "    \"Subject: Internet down | Body: My router is not working. -> Department: Technical Support\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUY4lNkV-ONS"
      },
      "source": [
        "Now we define the a function that gets an email subject and body and builds the prompt by adding the email content after the few-shot example prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBtqtgDHD-fY"
      },
      "outputs": [],
      "source": [
        "# Prompt building function\n",
        "\n",
        "def build_prompt(subject: str, body: str) -> str:\n",
        "    few_shot = prompt\n",
        "\n",
        "    return (\n",
        "        few_shot\n",
        "        + \"Subject: \" + (subject or \"\") + \" | \"\n",
        "        + \"Body: \" + (body or \"\") + \" -> \"\n",
        "        + \"Department:\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLpXP3RAEUaR"
      },
      "source": [
        "### Email Routing Function\n",
        "Next, we define the email routing function that gets the model it should use for routing and the email it should route. It builds the prompt using the build_prompt function to build the prompt, tokenizes it and then lets the model generate the output. Afterwards, it only extracts the corresponding department from the output and returns that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvl8wb0QrTm5"
      },
      "outputs": [],
      "source": [
        "# Email routing function, takes an email dict and returns the predicted department\n",
        "\n",
        "def route_email(model, dict_email):\n",
        "\n",
        "    if dict_email is None:\n",
        "        raise ValueError(\"Email is None\")\n",
        "\n",
        "    # Cut the email subject and body to a reasonable length\n",
        "\n",
        "    subject = (dict_email.get(\"subject\") or \"\")[:200]\n",
        "    body = (dict_email.get(\"body\") or \"\")[:1200]\n",
        "\n",
        "    # Build the prompt consisting of the few-shoot examples and the email that should be routed\n",
        "\n",
        "    prompt = build_prompt(subject, body)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "\n",
        "    inputs = tokenizer_gpt(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move inputs to same device as output\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5, # Generate at most 5 tokens\n",
        "            do_sample=False, # Determenistic decoding, no sampling\n",
        "            num_beams=3, # Use beam search with 3 beams to explore multiple best continuations\n",
        "            eos_token_id=tokenizer_gpt.eos_token_id,\n",
        "            pad_token_id=tokenizer_gpt.eos_token_id, # TODO\n",
        "        )\n",
        "\n",
        "    # Generate output\n",
        "\n",
        "    gen = tokenizer_gpt.decode(\n",
        "        out[0][inputs[\"input_ids\"].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    # Postprocess the output to extract the department\n",
        "\n",
        "    if \"Department:\" in gen:\n",
        "        gen = gen.split(\"Department:\", 1)[1].strip()\n",
        "\n",
        "    ALLOWED = [\n",
        "        \"Technical Support\",\n",
        "        \"Customer Service\",\n",
        "        \"Billing and Payments\",\n",
        "        \"Sales and Pre-Sales\",\n",
        "        \"General Inquiry\",\n",
        "    ]\n",
        "    for d in ALLOWED:\n",
        "        if gen.startswith(d):\n",
        "            return d\n",
        "\n",
        "    # Default to General Inquiry if no valid department found in the generated output\n",
        "    return \"General Inquiry\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP_fNfmSrrpl"
      },
      "source": [
        "### Evaluation Function\n",
        "The evaluation function runs through the whole test set and routes each email. During the runthrough it creates a confusion matrix from which it can also calculated the accuracy and F1 macro value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSDD9D82rtow"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model):\n",
        "\n",
        "  LABELS = [\n",
        "      \"Technical Support\",\n",
        "      \"Sales and Pre-Sales\",\n",
        "      \"Billing and Payments\",\n",
        "      \"Customer Service\",\n",
        "      \"General Inquiry\",\n",
        "  ]\n",
        "\n",
        "  label_to_idx = {l: i for i, l in enumerate(LABELS)}\n",
        "  n = len(LABELS)\n",
        "\n",
        "  confusion = np.zeros((n, n), dtype=int)\n",
        "\n",
        "  # Route every email in the test dataset and update the confusion matrix\n",
        "  for email in tqdm(test_ds):\n",
        "      y_true = label_to_idx[email[\"queue\"]]\n",
        "      y_pred = label_to_idx[route_email(model, email)]\n",
        "      confusion[y_true, y_pred] += 1\n",
        "\n",
        "  # Compute the accuracy\n",
        "  accuracy = np.trace(confusion) / confusion.sum()\n",
        "\n",
        "  # Compute the Macro F1 score\n",
        "  eps = 1e-12\n",
        "  f1s = []\n",
        "\n",
        "  for i in range(n):\n",
        "      tp = confusion[i, i]\n",
        "      fp = confusion[:, i].sum() - tp\n",
        "      fn = confusion[i, :].sum() - tp\n",
        "\n",
        "      precision = tp / (tp + fp + eps)\n",
        "      recall = tp / (tp + fn + eps)\n",
        "      f1s.append(2 * precision * recall / (precision + recall + eps))\n",
        "\n",
        "  macro_f1 = float(np.mean(f1s))\n",
        "\n",
        "  # Put out the accuracy, macro F1 and the confusion matrix as a heatmap\n",
        "  print(f\"Accuracy:  {accuracy:.2%}\")\n",
        "  print(f\"Macro F1:  {macro_f1:.3f}\")\n",
        "\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  sns.heatmap(\n",
        "      confusion,\n",
        "      annot=True,\n",
        "      fmt=\"d\",\n",
        "      cmap=\"Blues\",\n",
        "      xticklabels=LABELS,\n",
        "      yticklabels=LABELS,\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"True\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJSV8dfCtSWV"
      },
      "source": [
        "### Evaluation\n",
        "Here we call the previously defined evaluation function on both models to compute the performance results for the first task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pH3fec1tUYm"
      },
      "outputs": [],
      "source": [
        "# evaluate_model(distilgpt2_model)\n",
        "# evaluate_model(gpt2_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3avdv5DYLtw"
      },
      "source": [
        "### Evaluation: (Screenshots because the model evaluation takes about 30min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nfJdOMiYMiE"
      },
      "outputs": [],
      "source": [
        "_ = plot_grafics(\"Screenshots/cm_base_gpt2.png\", \"Screenshots/cm_base_distilgpt2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwqdXe3VzfTT"
      },
      "source": [
        "## Agent 2: Routing with fine-tuning (LoRA) on GPT-2 and DistilGPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK69rwJwNwq_"
      },
      "source": [
        "### LoRa configuration\n",
        "Here we determine the configurations for the LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjYIFNOkNtvY"
      },
      "outputs": [],
      "source": [
        "# Configure the Lora\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8, # Rank of the LoRA update matrices (higher = more capacity, more trainable params)\n",
        "    lora_alpha=16,# Scaling factor for LoRA updates (controls how strongly LoRA affects the base weights)\n",
        "    lora_dropout=0.05, # Dropout applied to LoRA layers during training (helps reduce overfitting)\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # Apply LoRA only to these GPT-2 attention/projection layers\n",
        "    bias=\"none\", # Do not train/add bias parameters (keeps the number of trainable params smaller)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1q0jnLH9m6M"
      },
      "source": [
        "### Model initialization function\n",
        "This function initiates the model for training. It gets a model name and first loads the pretrained model and sets the pad/eos token IDs to match the tokenizer to avoid token-mismatch issues during batching and loss computation Then it wrap the model with PEFT LoRA adapters and sets it to training mode. The model is the returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOphk9TU9pT8"
      },
      "outputs": [],
      "source": [
        "def initiate_model_for_training(model_name):\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  model.config.pad_token_id = tokenizer_gpt.pad_token_id\n",
        "  model.config.eos_token_id = tokenizer_gpt.eos_token_id\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERmi7MWK3MSr"
      },
      "source": [
        "### Data preprocessing functions\n",
        "The tokenize function formats each email into a promptâ€“target pair, tokenizes both parts, truncates to a fixed maximum length (256 tokens), and masks the prompt tokens in the labels so that the model is trained to predict only the classification label in a causal language modeling setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okxWstTL7JGa"
      },
      "outputs": [],
      "source": [
        "def tokenize(email):\n",
        "\n",
        "    # Truncate subject and body to limit extremely long inputs\n",
        "    # (helps control total sequence length and memory usage)\n",
        "    subject = str(email[\"subject\"])[:200]\n",
        "    body = str(email[\"body\"])[:1200]\n",
        "    label = str(email[\"label\"]).strip()\n",
        "\n",
        "    # Build the prompt\n",
        "    prompt = build_prompt(subject, body)\n",
        "\n",
        "    # The target is the gold label followed by EOS token.\n",
        "    # Leading space ensures proper tokenization for GPT-style models.\n",
        "    target = \" \" + label + tokenizer_gpt.eos_token\n",
        "\n",
        "    # Tokenize prompt and target separately without adding extra special tokens\n",
        "    # (we control EOS manually)\n",
        "    prompt_ids = tokenizer_gpt(prompt, add_special_tokens=False)[\"input_ids\"]\n",
        "    target_ids = tokenizer_gpt(target, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Ensure total length does not exceed model limit (256 tokens here)\n",
        "    # We reserve space for the target and truncate the prompt if needed\n",
        "    max_prompt_len = 256 - len(target_ids)\n",
        "\n",
        "    if max_prompt_len < 1:\n",
        "        # If the target itself exceeds max length,\n",
        "        # truncate the target and discard the prompt\n",
        "        target_ids = target_ids[:256]\n",
        "        prompt_ids = []\n",
        "    else:\n",
        "        # Otherwise truncate the prompt to fit within max length\n",
        "        prompt_ids = prompt_ids[:max_prompt_len]\n",
        "\n",
        "    # Final input is prompt followed by target (standard causal LM setup)\n",
        "    input_ids = prompt_ids + target_ids\n",
        "\n",
        "    # Attention mask: 1 for real tokens (no padding used here)\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Labels:\n",
        "    # - Ignore prompt tokens in the loss by setting them to -100\n",
        "    # - Compute loss only on target tokens (the label)\n",
        "    labels = [-100] * len(prompt_ids) + target_ids\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2gVRQtG3rr"
      },
      "source": [
        "The preprocessing function gets a dataset and first standardizes the dataset by renaming the queue column to label. It then removes all unnecessary columns and tokenizes the data into model-ready inputs (input IDs, attention masks, and labels) for causal language model training. Then it returns the ready dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1P-Yec93Rj9"
      },
      "outputs": [],
      "source": [
        "# Renaming function\n",
        "\n",
        "def rename_queue_to_label(ex):\n",
        "    lab = ex[\"queue\"]\n",
        "    if lab is None:\n",
        "        lab = \"\"\n",
        "    return {\"label\": str(lab).strip()}\n",
        "\n",
        "# Preprocessing function\n",
        "\n",
        "def preprocess_for_training(dataset):\n",
        "\n",
        "  keep_cols = [\"subject\", \"body\", \"queue\"]\n",
        "  dataset = dataset.remove_columns([c for c in dataset.column_names if c not in keep_cols])\n",
        "  dataset = dataset.map(rename_queue_to_label)\n",
        "  dataset = dataset.remove_columns(['queue'])\n",
        "  tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "  return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYyOooWg8DUo"
      },
      "source": [
        "### Training configuration\n",
        "We configure the training arguments here. We used a per-device batch size of 2 with gradient accumulation over 8 steps (effective batch size = 16). The model was trained for 10 epochs with a learning rate of 3e-4. Evaluation and checkpointing were conducted at the end of each epoch, and up to 10 checkpoints were retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFGVo9v78J9x"
      },
      "outputs": [],
      "source": [
        "# Set the training arguments\n",
        "def make_training_args(model_name):\n",
        "    output_path = f\"{model_name}_fine-tuned\"\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir= output_path, # Directory to save checkpoints and logs\n",
        "        per_device_train_batch_size=2, # Batch size per GPU/CPU device\n",
        "        gradient_accumulation_steps=8,  # Accumulate gradients to simulate a larger batch (effective batch = 2*8 = 16)\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=3e-4,\n",
        "\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=10,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyntuOa1-bCA"
      },
      "source": [
        "### Batching function\n",
        "The collate_fn dynamically pads all sequences in a batch to the length of the longest example in that batch and converts them into PyTorch tensors. Padding tokens are masked in both the attention mask (0) and the loss computation (-100 in labels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA2vD7IF8VkR"
      },
      "outputs": [],
      "source": [
        "# Custom collate function for dynamic padding within each batch\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer_gpt.pad_token_id\n",
        "\n",
        "    # Determine the maximum sequence length in the current batch\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "\n",
        "    for x in batch:\n",
        "        ids, att, lab = x[\"input_ids\"], x[\"attention_mask\"], x[\"labels\"]\n",
        "\n",
        "        # Compute how much padding is needed for this example\n",
        "        pad_len = max_len - len(ids)\n",
        "\n",
        "        # Pad input_ids with pad token ID\n",
        "        input_ids.append(ids + [pad_id] * pad_len)\n",
        "\n",
        "        # Pad attention_mask with 0 (0 = ignore padding tokens)\n",
        "        attention_mask.append(att + [0] * pad_len)\n",
        "\n",
        "        # Pad labels with -100 so padding tokens are ignored in loss computation\n",
        "        labels.append(lab + [-100] * pad_len)\n",
        "\n",
        "    # Convert lists into PyTorch tensors for model input\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpZo7wZ1-zqC"
      },
      "source": [
        "### Trainer initialization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtBZwWQt-3No"
      },
      "outputs": [],
      "source": [
        "def initialize_trainer(model, train_dataset, eval_dataset, args):\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      data_collator=collate_fn\n",
        "  )\n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9-VG83GByLD"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyVpt_tDBw8z"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "\n",
        "model_for_training_distilgpt2 = initiate_model_for_training(\"distilgpt2\")\n",
        "model_for_training_gpt2 = initiate_model_for_training(\"gpt2\")\n",
        "\n",
        "# Preprocess and tokenize datasets\n",
        "tokenized_train = preprocess_for_training(train_ds)\n",
        "tokenized_val = preprocess_for_training(val_ds)\n",
        "\n",
        "# Make training arguments\n",
        "training_args_gpt2 = make_training_args(\"gpt2\")\n",
        "training_args_distilgpt2 = make_training_args(\"distilgpt2\")\n",
        "\n",
        "# Initialize trainer\n",
        "\n",
        "trainer_gpt2 = initialize_trainer(model_for_training_gpt2, tokenized_train, tokenized_val, training_args_gpt2)\n",
        "trainer_distilgpt2 = initialize_trainer(model_for_training_distilgpt2, tokenized_train, tokenized_val, training_args_distilgpt2)\n",
        "\n",
        "# Train models\n",
        "\n",
        "#trainer_gpt2.train()\n",
        "#model_fine_tuned_gpt2 = trainer_gpt2.model\n",
        "#trainer_gpt2.save_model(\"gpt2_fine-tuned\")\n",
        "\n",
        "#trainer_distilgpt2.train()\n",
        "#trainer_distilgpt2.save_model(\"distilgpt2_fine-tuned\")\n",
        "#model_fine_tuned_distilgpt2 = trainer_distilgpt2.model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y280O7YPOUHI"
      },
      "source": [
        "### Load trained models from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq35qo9sOZJg"
      },
      "outputs": [],
      "source": [
        "#model_fine_tuned_gpt2 = PeftModel.from_pretrained(gpt2_model, 'gpt2_fine-tuned')\n",
        "#model_fine_tuned_gpt2.eval()\n",
        "\n",
        "\n",
        "#model_fine_tuned_distilgpt2 = PeftModel.from_pretrained(distilgpt2_model, 'distilgpt2_fine-tuned')\n",
        "#model_fine_tuned_distilgpt2.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVV5VH7UN8h"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5OpjA8YUNEr"
      },
      "outputs": [],
      "source": [
        "#evaluate_model(model_fine_tuned_gpt2)\n",
        "#evaluate_model(model_fine_tuned_distilgpt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUa-MtgZYm7q"
      },
      "source": [
        "### Evaluation: (Screenshots because the model evaluation takes about 1:15h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqmQslTQYnx1"
      },
      "outputs": [],
      "source": [
        "_ = plot_grafics(\"Screenshots/cm_trained_gpt2.jpeg\", \"Screenshots/cm_trained_distilgpt2.jpeg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrEcxtn2WQ4N"
      },
      "source": [
        "## Agent 3: Routing with discriminative classifier using a BERT-like model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDYp3K_WWW4k"
      },
      "source": [
        "### Setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHLzVTiTqrN1"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "bert_checkpoint = \"distilbert-base-uncased\"\n",
        "output_dir = \"./distilbert-checkpoint\"\n",
        "\n",
        "# Loading tokenizer\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FZ6ZN47rQvY"
      },
      "source": [
        "### Preprocessing\n",
        "The preprocessing function helps handling empty inputs and applies tokenization with padding/truncation to format inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCZG-KTXrQiK"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_bert(examples):\n",
        "    # Making sure all inputs are strings\n",
        "    subjects = [str(s) if s is not None else \"\" for s in examples[\"subject\"]]\n",
        "    bodies = [str(b) if b is not None else \"\" for b in examples[\"body\"]]\n",
        "\n",
        "    # Tokenization\n",
        "    return bert_tokenizer(\n",
        "        subjects,\n",
        "        bodies,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKTm3bTsrGf7"
      },
      "source": [
        "### Tokenization\n",
        "Executing the tokenization and mapping the labels to integers. Finally, we set the format to 'torch' to prepare the necessary columns (input_ids, attention_mask, label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pzI_nXgcrGOP"
      },
      "outputs": [],
      "source": [
        "print(\"Tokenizing datasets.\")\n",
        "encoded_train = train_ds.map(preprocess_bert, batched=True)\n",
        "encoded_val = val_ds.map(preprocess_bert, batched=True)\n",
        "encoded_test = test_ds.map(preprocess_bert, batched=True)\n",
        "\n",
        "# Label Formatting\n",
        "def format_labels(example):\n",
        "    return {\"label\": label2id[example[\"queue\"]]}\n",
        "\n",
        "# Label mapping\n",
        "bert_train_ds = encoded_train.map(format_labels)\n",
        "bert_val_ds = encoded_val.map(format_labels)\n",
        "bert_test_ds = encoded_test.map(format_labels)\n",
        "\n",
        "# Set columns\n",
        "cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "bert_train_ds.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "bert_val_ds.set_format(type=\"torch\", columns=cols_to_keep)\n",
        "bert_test_ds.set_format(type=\"torch\", columns=cols_to_keep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWqMLPNTr0-Y"
      },
      "source": [
        "### Defining metrics and training arguments\n",
        "Here we define the accuracy metric to evaluate the model. We then load the pre-trained DistilBERT model and modify it to output predictions for our number of labels. Finally, we set the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bnp1bAjhWWpr"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# Loading model\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    bert_checkpoint,\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
        "\n",
        "# Training Arguments\n",
        "bert_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=4, # 10 was overfitting\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H8DMecosEFF"
      },
      "source": [
        "### Training and Saving the model\n",
        "Finally, we bring everything together by initializing the Trainer. We pass it the model, the arguments, the datasets, and the data collator. The commands below (currently commented out) are used to start the actual fine-tuning process and to save the trained model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L30-p3I7sDZn"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "bert_trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=bert_args,\n",
        "    train_dataset=bert_train_ds,\n",
        "    eval_dataset=bert_val_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# print(\"Starting Training DistilBERT:\")\n",
        "# bert_trainer.train()\n",
        "\n",
        "# # Saving the model\n",
        "# save_path = \"./distilbert-path\"\n",
        "\n",
        "# bert_trainer.save_model(save_path)\n",
        "# bert_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z92Dwhm2Wa08"
      },
      "source": [
        "### Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILAJErvXWdMg"
      },
      "outputs": [],
      "source": [
        "# Loading the BERT model\n",
        "#model_bert_path = \"./distilbert-final\"\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "#model_bert = AutoModelForSequenceClassification.from_pretrained(model_bert_path) # AutoModelForSequenceClassification because it's a classifier\n",
        "#tokenize_bert = AutoTokenizer.from_pretrained(model_bert_path)\n",
        "\n",
        "#model_bert.to(device)\n",
        "#model_bert.eval()\n",
        "\n",
        "#print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mte4XM5RWdnS"
      },
      "source": [
        "### Evaluation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRBdpwx0Wf7g"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function for DistilBERT\n",
        "def evaluate_bert_model(model, tokenizer, dataset, name=\"DistilBERT\"):\n",
        "    # Evaluation (Accuracy, Macro-F1, Macro-Recall, Confusion Matrix)\n",
        "    print(f\"Evaluating {name}:\")\n",
        "\n",
        "    # Evaluation Trainer Setup\n",
        "    eval_trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=TrainingArguments(\n",
        "            output_dir=\"./eval_results\",\n",
        "            report_to=\"none\",\n",
        "            per_device_eval_batch_size=16\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Generating predictions\n",
        "    output = eval_trainer.predict(dataset)\n",
        "\n",
        "    # Getting predicted and true labels\n",
        "    y_pred = np.argmax(output.predictions, axis=1)\n",
        "    y_true = output.label_ids\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    # Labels\n",
        "    ALLOWED_CLASSES = [\n",
        "        \"Technical Support\",\n",
        "        \"Sales and Pre-Sales\",\n",
        "        \"Billing and Payments\",\n",
        "        \"Customer Service\",\n",
        "        \"General Inquiry\"\n",
        "    ]\n",
        "\n",
        "    # Label IDs\n",
        "    label2id = model.config.label2id\n",
        "    custom_ids = [label2id[lab] for lab in ALLOWED_CLASSES]\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=custom_ids)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ALLOWED_CLASSES)\n",
        "    disp.plot(cmap=\"Blues\", ax=ax, xticks_rotation='vertical')\n",
        "    plt.title(f\"Confusion Matrix - {name}\\n(Acc: {acc:.1%} | F1: {f1:.1%} | Recall: {recall:.1%})\")\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig(f\"confusion_matrix_{name}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Output metrics\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"recall\": recall}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AxzYys0WnjO"
      },
      "source": [
        "### Evaluation: (Screenshots because the model is not loadable in colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn1DXKehWpY-"
      },
      "outputs": [],
      "source": [
        "# Evaluation of DistilBERT\n",
        "#_ = evaluate_bert_model(model_bert, tokenize_bert, bert_test_ds, name=\"DistilBERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpY7tjnHW8OZ"
      },
      "source": [
        "Evaluation Results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4sEgCRSYKYe"
      },
      "outputs": [],
      "source": [
        "# Display Results\n",
        "_ = plot_grafics(\"Screenshots/confusion_matrix_DistilBERT.png\", \"Screenshots/confusion_matrix_DistilBERT (overfitted).png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIMIkgC_Tlln"
      },
      "source": [
        "## Overall Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAakUqyGaqpW"
      },
      "source": [
        "### Performance results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srRDcVkCaqD7"
      },
      "outputs": [],
      "source": [
        "_ = show_results_table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ5LcdoqVOHR"
      },
      "source": [
        "### Confusion Matrices per model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCfWcJSCT1gB"
      },
      "outputs": [],
      "source": [
        "_ = plot_grafics(\"Screenshots/cm_base_gpt2.png\", \"Screenshots/cm_base_distilgpt2.png\")\n",
        "_ = plot_grafics(\"Screenshots/cm_trained_gpt2.jpeg\", \"Screenshots/cm_trained_distilgpt2.jpeg\")\n",
        "_ = plot_grafics(\"Screenshots/confusion_matrix_DistilBERT.png\", \"Screenshots/confusion_matrix_DistilBERT (overfitted).png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LERvjo1UB79M",
        "tr42eipZzP71",
        "qLpXP3RAEUaR",
        "iP_fNfmSrrpl",
        "eJSV8dfCtSWV",
        "bK69rwJwNwq_",
        "E1q0jnLH9m6M",
        "ERmi7MWK3MSr",
        "nYyOooWg8DUo",
        "cpZo7wZ1-zqC",
        "g9-VG83GByLD",
        "Y280O7YPOUHI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "build-nanogpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}