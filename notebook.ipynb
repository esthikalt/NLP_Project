{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LERvjo1UB79M",
        "tr42eipZzP71",
        "qLpXP3RAEUaR",
        "iP_fNfmSrrpl",
        "eJSV8dfCtSWV",
        "bK69rwJwNwq_",
        "E1q0jnLH9m6M",
        "ERmi7MWK3MSr",
        "nYyOooWg8DUo",
        "cpZo7wZ1-zqC",
        "g9-VG83GByLD",
        "Y280O7YPOUHI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Project - Jonas Luerkens & Esther Kaltwasser"
      ],
      "metadata": {
        "id": "rOL2nU8W94Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "esVZLd73Bb0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable access to the drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to project directory\n",
        "\n",
        "project_directory = '/content/drive/MyDrive/Colab\\ Notebooks/NLP'\n",
        "\n",
        "# Go to project directory\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/NLP\n",
        "\n",
        "# Install requirements\n",
        "\n",
        "#!pip install -r $project_directory/requirements.txt"
      ],
      "metadata": {
        "id": "okMoSeaGOUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjqDNEzD9g0m"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "from datapreparation import load_and_prepare_data\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "train_ds, val_ds, test_ds, label_list, label2id, id2label = load_and_prepare_data()"
      ],
      "metadata": {
        "id": "ajaAd5jVBEdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models and tokenizer for task 1 and 2\n",
        "\n",
        "distilgpt2_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\") # Same tokenizer as distilgpt2\n",
        "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token"
      ],
      "metadata": {
        "id": "d9kpcv2dLH7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HIER SET DEVICE"
      ],
      "metadata": {
        "id": "g3yEkhmVCNQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics can vary across runs due to stochastic training;\n",
        "# Setting seeds helps ensure that reported results can be replicated\n",
        "# It fixes the random number generators so the model initializes and trains in the same way each time\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "id": "9yZocsEiB_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 1: Routing with prompting using GPT-2 (frozen model)"
      ],
      "metadata": {
        "id": "LERvjo1UB79M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt function and prompt\n",
        "Here we define the few-shot prompt for the model. It includes one short, precise example for one email for each department."
      ],
      "metadata": {
        "id": "tr42eipZzP71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot learning prompt (1 email per department)\n",
        "\n",
        "prompt = (\n",
        "    \"Classify the email ONLY into the following categories: Technical Support, Customer Service, Billing and Payments, Sales and Pre-Sales, General Inquiry.\\n\\n\"\n",
        "    \"Subject: Return item | Body: I want to send this product back. -> Department: Customer Service\\n\"\n",
        "    \"Subject: Invoice missing | Body: I need my bill for July. -> Department: Billing and Payments\\n\"\n",
        "    \"Subject: Discount inquiry | Body: Do you have bulk pricing? -> Department: Sales and Pre-Sales\\n\"\n",
        "    \"Subject: Opening hours | Body: When are you open? -> Department: General Inquiry\\n\"\n",
        "    \"Subject: Internet down | Body: My router is not working. -> Department: Technical Support\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "u1Yy1newBpKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the a function that gets an email subject and body and builds the prompt by adding the email content after the few-shot example prompt."
      ],
      "metadata": {
        "id": "oUY4lNkV-ONS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt building function\n",
        "\n",
        "def build_prompt(subject: str, body: str) -> str:\n",
        "    few_shot = prompt\n",
        "\n",
        "    return (\n",
        "        few_shot\n",
        "        + \"Subject: \" + (subject or \"\") + \" | \"\n",
        "        + \"Body: \" + (body or \"\") + \" -> \"\n",
        "        + \"Department:\"\n",
        "    )"
      ],
      "metadata": {
        "id": "bBtqtgDHD-fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Email Routing Function\n",
        "Next, we define the email routing function that gets the model it should use for routing and the email it should route. It builds the prompt using the build_prompt function to build the prompt, tokenizes it and then lets the model generate the output. Afterwards, it only extracts the corresponding department from the output and returns that."
      ],
      "metadata": {
        "id": "qLpXP3RAEUaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Email routing function, takes an email dict and returns the predicted department\n",
        "\n",
        "def route_email(model, dict_email):\n",
        "\n",
        "    if dict_email is None:\n",
        "        raise ValueError(\"Email is None\")\n",
        "\n",
        "    # Cut the email subject and body to a reasonable length\n",
        "\n",
        "    subject = (dict_email.get(\"subject\") or \"\")[:200]\n",
        "    body = (dict_email.get(\"body\") or \"\")[:1200]\n",
        "\n",
        "    # Build the prompt consisting of the few-shoot examples and the email that should be routed\n",
        "\n",
        "    prompt = build_prompt(subject, body)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "\n",
        "    inputs = tokenizer_gpt(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move inputs to same device as output\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5, # Generate at most 5 tokens\n",
        "            do_sample=False, # Determenistic decoding, no sampling\n",
        "            num_beams=3, # Use beam search with 3 beams to explore multiple best continuations\n",
        "            eos_token_id=tokenizer_gpt.eos_token_id,\n",
        "            pad_token_id=tokenizer_gpt.eos_token_id, # TODO\n",
        "        )\n",
        "\n",
        "    # Generate output\n",
        "\n",
        "    gen = tokenizer_gpt.decode(\n",
        "        out[0][inputs[\"input_ids\"].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    # Postprocess the output to extract the department\n",
        "\n",
        "    if \"Department:\" in gen:\n",
        "        gen = gen.split(\"Department:\", 1)[1].strip()\n",
        "\n",
        "    ALLOWED = [\n",
        "        \"Technical Support\",\n",
        "        \"Customer Service\",\n",
        "        \"Billing and Payments\",\n",
        "        \"Sales and Pre-Sales\",\n",
        "        \"General Inquiry\",\n",
        "    ]\n",
        "    for d in ALLOWED:\n",
        "        if gen.startswith(d):\n",
        "            return d\n",
        "\n",
        "    # Default to General Inquiry if no valid department found in the generated output\n",
        "    return \"General Inquiry\"\n"
      ],
      "metadata": {
        "id": "dvl8wb0QrTm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Function\n",
        "The evaluation function runs through the whole test set and routes each email. During the runthrough it creates a confusion matrix from which it can also calculated the accuracy and F1 macro value"
      ],
      "metadata": {
        "id": "iP_fNfmSrrpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "\n",
        "  LABELS = [\n",
        "      \"Technical Support\",\n",
        "      \"Sales and Pre-Sales\",\n",
        "      \"Billing and Payments\",\n",
        "      \"Customer Service\",\n",
        "      \"General Inquiry\",\n",
        "  ]\n",
        "\n",
        "  label_to_idx = {l: i for i, l in enumerate(LABELS)}\n",
        "  n = len(LABELS)\n",
        "\n",
        "  confusion = np.zeros((n, n), dtype=int)\n",
        "\n",
        "  # Route every email in the test dataset and update the confusion matrix\n",
        "  for email in test_ds:\n",
        "      y_true = label_to_idx[email[\"queue\"]]\n",
        "      y_pred = label_to_idx[route_email(model, email)]\n",
        "      confusion[y_true, y_pred] += 1\n",
        "\n",
        "  # Compute the accuracy\n",
        "  accuracy = np.trace(confusion) / confusion.sum()\n",
        "\n",
        "  # Compute the Macro F1 score\n",
        "  eps = 1e-12\n",
        "  f1s = []\n",
        "\n",
        "  for i in range(n):\n",
        "      tp = confusion[i, i]\n",
        "      fp = confusion[:, i].sum() - tp\n",
        "      fn = confusion[i, :].sum() - tp\n",
        "\n",
        "      precision = tp / (tp + fp + eps)\n",
        "      recall = tp / (tp + fn + eps)\n",
        "      f1s.append(2 * precision * recall / (precision + recall + eps))\n",
        "\n",
        "  macro_f1 = float(np.mean(f1s))\n",
        "\n",
        "  # Put out the accuracy, macro F1 and the confusion matrix as a heatmap\n",
        "  print(f\"Accuracy:  {accuracy:.2%}\")\n",
        "  print(f\"Macro F1:  {macro_f1:.3f}\")\n",
        "\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  sns.heatmap(\n",
        "      confusion,\n",
        "      annot=True,\n",
        "      fmt=\"d\",\n",
        "      cmap=\"Blues\",\n",
        "      xticklabels=LABELS,\n",
        "      yticklabels=LABELS,\n",
        "  )\n",
        "\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"True\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "PSDD9D82rtow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "Here we call the previously defined evaluation funcion on both model to compute the performance results for the first task"
      ],
      "metadata": {
        "id": "eJSV8dfCtSWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(distilgpt2_model)\n",
        "evaluate_model(gpt2_model)\n"
      ],
      "metadata": {
        "id": "9pH3fec1tUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 2: Routing with fine-tuning (LoRA) on GPT-2 and DistilGPT2"
      ],
      "metadata": {
        "id": "iwqdXe3VzfTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRa configuration\n",
        "Here we determine the configurations for the LoRA"
      ],
      "metadata": {
        "id": "bK69rwJwNwq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the Lora\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8, # Rank of the LoRA update matrices (higher = more capacity, more trainable params)\n",
        "    lora_alpha=16,# Scaling factor for LoRA updates (controls how strongly LoRA affects the base weights)\n",
        "    lora_dropout=0.05, # Dropout applied to LoRA layers during training (helps reduce overfitting)\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # Apply LoRA only to these GPT-2 attention/projection layers\n",
        "    bias=\"none\", # Do not train/add bias parameters (keeps the number of trainable params smaller)\n",
        ")\n"
      ],
      "metadata": {
        "id": "wjYIFNOkNtvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model initialization function\n",
        "This function initiates the model for training. It gets a model name and first loads the pretrained model and sets the pad/eos token IDs to match the tokenizer to avoid token-mismatch issues during batching and loss computation Then it wrap the model with PEFT LoRA adapters and sets it to training mode. The model is the returned"
      ],
      "metadata": {
        "id": "E1q0jnLH9m6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initiate_model_for_training(model_name):\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  model.config.pad_token_id = tokenizer_gpt.pad_token_id\n",
        "  model.config.eos_token_id = tokenizer_gpt.eos_token_id\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "GOphk9TU9pT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing functions\n",
        "The tokenize function formats each email into a promptâ€“target pair, tokenizes both parts, truncates to a fixed maximum length (256 tokens), and masks the prompt tokens in the labels so that the model is trained to predict only the classification label in a causal language modeling setup.\n"
      ],
      "metadata": {
        "id": "ERmi7MWK3MSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(email):\n",
        "\n",
        "    # Truncate subject and body to limit extremely long inputs\n",
        "    # (helps control total sequence length and memory usage)\n",
        "    subject = str(email[\"subject\"])[:200]\n",
        "    body = str(email[\"body\"])[:1200]\n",
        "    label = str(email[\"label\"]).strip()\n",
        "\n",
        "    # Build the prompt\n",
        "    prompt = build_prompt(subject, body)\n",
        "\n",
        "    # The target is the gold label followed by EOS token.\n",
        "    # Leading space ensures proper tokenization for GPT-style models.\n",
        "    target = \" \" + label + tokenizer_gpt.eos_token\n",
        "\n",
        "    # Tokenize prompt and target separately without adding extra special tokens\n",
        "    # (we control EOS manually)\n",
        "    prompt_ids = tokenizer_gpt(prompt, add_special_tokens=False)[\"input_ids\"]\n",
        "    target_ids = tokenizer_gpt(target, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Ensure total length does not exceed model limit (256 tokens here)\n",
        "    # We reserve space for the target and truncate the prompt if needed\n",
        "    max_prompt_len = 256 - len(target_ids)\n",
        "\n",
        "    if max_prompt_len < 1:\n",
        "        # If the target itself exceeds max length,\n",
        "        # truncate the target and discard the prompt\n",
        "        target_ids = target_ids[:256]\n",
        "        prompt_ids = []\n",
        "    else:\n",
        "        # Otherwise truncate the prompt to fit within max length\n",
        "        prompt_ids = prompt_ids[:max_prompt_len]\n",
        "\n",
        "    # Final input is prompt followed by target (standard causal LM setup)\n",
        "    input_ids = prompt_ids + target_ids\n",
        "\n",
        "    # Attention mask: 1 for real tokens (no padding used here)\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Labels:\n",
        "    # - Ignore prompt tokens in the loss by setting them to -100\n",
        "    # - Compute loss only on target tokens (the label)\n",
        "    labels = [-100] * len(prompt_ids) + target_ids\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n"
      ],
      "metadata": {
        "id": "okxWstTL7JGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing function gets a dataset and first standardizes the dataset by renaming the queue column to label. It then removes all unnecessary columns and tokenizes the data into model-ready inputs (input IDs, attention masks, and labels) for causal language model training. Then it returns the ready dataset"
      ],
      "metadata": {
        "id": "iP2gVRQtG3rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming function\n",
        "\n",
        "def rename_queue_to_label(ex):\n",
        "    lab = ex[\"queue\"]\n",
        "    if lab is None:\n",
        "        lab = \"\"\n",
        "    return {\"label\": str(lab).strip()}\n",
        "\n",
        "# Preprocessing function\n",
        "\n",
        "def preprocess_for_training(dataset):\n",
        "\n",
        "  keep_cols = [\"subject\", \"body\", \"queue\"]\n",
        "  dataset = dataset.remove_columns([c for c in dataset.column_names if c not in keep_cols])\n",
        "  dataset = dataset.map(rename_queue_to_label)\n",
        "  dataset = dataset.remove_columns(['queue'])\n",
        "  tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "F1P-Yec93Rj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training configuration\n",
        "We configure the training arguments here. We used a per-device batch size of 2 with gradient accumulation over 8 steps (effective batch size = 16). The model was trained for 10 epochs with a learning rate of 3e-4. Evaluation and checkpointing were conducted at the end of each epoch, and up to 10 checkpoints were retained. This enabled the comparison of the performance over epochs"
      ],
      "metadata": {
        "id": "nYyOooWg8DUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the training arguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='output_training',\n",
        "    per_device_train_batch_size=2, # Batch size per GPU/CPU device\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate a larger batch (effective batch = 2*8 = 16)\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=3e-4,\n",
        "\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=10,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "MFGVo9v78J9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define batching function"
      ],
      "metadata": {
        "id": "xyntuOa1-bCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batching function\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer_gpt.pad_token_id\n",
        "\n",
        "    # Find the longest sequence and pad everything else in the batch to the same size\n",
        "\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "    for x in batch:\n",
        "        ids, att, lab = x[\"input_ids\"], x[\"attention_mask\"], x[\"labels\"]\n",
        "        pad_len = max_len - len(ids)\n",
        "        input_ids.append(ids + [pad_id] * pad_len)\n",
        "        attention_mask.append(att + [0] * pad_len)\n",
        "        labels.append(lab + [-100] * pad_len)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "    }"
      ],
      "metadata": {
        "id": "kA2vD7IF8VkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define initialization function for trainer"
      ],
      "metadata": {
        "id": "cpZo7wZ1-zqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_trainer(model, train_dataset, eval_dataset):\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      data_collator=collate_fn\n",
        "  )\n"
      ],
      "metadata": {
        "id": "EtBZwWQt-3No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "g9-VG83GByLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "\n",
        "model_for_training_distilgpt2 = initiate_model_for_training(\"distilgpt2\")\n",
        "model_for_training_gpt2 = initiate_model_for_training(\"gpt2\")\n",
        "\n",
        "# Preprocess and tokenize datasets\n",
        "tokenized_train = preprocess_for_training(train_ds)\n",
        "tokenized_val = preprocess_for_training(val_ds)\n",
        "\n",
        "# Initialize trainer\n",
        "\n",
        "trainer_gpt2 = initialize_trainer(model_for_training_gpt2, tokenized_train, tokenized_val)\n",
        "trainer_distilgpt2 = initialize_trainer(model_for_training_distilgpt2, tokenized_train, tokenized_val)\n",
        "\n",
        "# Train models\n",
        "\n",
        "#trainer_gpt2.train()\n",
        "#model_fine_tuned_gpt2 = trainer_gpt2.model\n",
        "\n",
        "#trainer_distilgpt2.train()\n",
        "#model_fine_tuned_distilgpt2 = trainer_distilgpt2.model\n"
      ],
      "metadata": {
        "id": "qyVpt_tDBw8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load trained models from directory"
      ],
      "metadata": {
        "id": "Y280O7YPOUHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tuned_gpt2 = PeftModel.from_pretrained(gpt2_model, 'gpt2_fine-tuned')\n",
        "model_fine_tuned_gpt2.eval()\n",
        "\n",
        "model_fine_tuned_distilgpt2 = PeftModel.from_pretrained(distilgpt2_model, 'distilgpt2_fine-tuned')\n",
        "model_fine_tuned_distilgpt2.eval()"
      ],
      "metadata": {
        "id": "Oq35qo9sOZJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "ADVV5VH7UN8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model_fine_tuned_gpt2)\n",
        "evaluate_model(model_fine_tuned_distilgpt2)"
      ],
      "metadata": {
        "id": "m5OpjA8YUNEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}